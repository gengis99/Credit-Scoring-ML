# -*- coding: utf-8 -*-
"""credit-scoring.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NhgQBpGkwvM-CzttHGiqs0uAfEGryEeA
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, brier_score_loss

# 1) Load
path = "/german.data"
df = pd.read_csv(path, sep=r"\s+", header=None)

# 2) Column names (da german.doc)
cols = [
    "checking_status", "duration_months", "credit_history", "purpose", "credit_amount",
    "savings_status", "employment_since", "installment_rate", "personal_status_sex",
    "other_debtors", "residence_since", "property", "age", "other_installment_plans",
    "housing", "existing_credits", "job", "num_liable", "telephone", "foreign_worker",
    "label"
]
df.columns = cols

# 3) Target: positivo = BAD (2)
y = (df["label"] == 2).astype(int)
X = df.drop(columns=["label"])

# 4) Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# 5) Preprocessing
categorical = [
    "checking_status","credit_history","purpose","savings_status","employment_since",
    "personal_status_sex","other_debtors","property","other_installment_plans",
    "housing","job","telephone","foreign_worker"
]
numeric = [c for c in X.columns if c not in categorical]

preprocess = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical),
        ("num", Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler())
        ]), numeric)
    ],
    remainder="drop"
)

# 6) Baseline model
model = Pipeline(steps=[
    ("prep", preprocess),
    ("clf", LogisticRegression(max_iter=500, class_weight="balanced"))
])

model.fit(X_train, y_train)
p = model.predict_proba(X_test)[:, 1]

print("ROC-AUC:", roc_auc_score(y_test, p))
print("PR-AUC:", average_precision_score(y_test, p))
print("Brier:", brier_score_loss(y_test, p))

# 7) Cost-based thresholding (cost matrix: FN=5, FP=1)
# y=1 is BAD. Predict BAD if p >= t.
def expected_cost(y_true, y_prob, t, fp_cost=1, fn_cost=5):
    y_pred = (y_prob >= t).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return fp_cost * fp + fn_cost * fn

ts = np.linspace(0.01, 0.99, 99)
costs = [expected_cost(y_test, p, t) for t in ts]
t_star = ts[int(np.argmin(costs))]
print("Best threshold (min expected cost):", t_star, "Cost:", min(costs))

from sklearn.metrics import confusion_matrix, classification_report

t_star = 0.46
y_pred = (p >= t_star).astype(int)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

print("TN FP FN TP:", tn, fp, fn, tp)
print("Cost:", fp + 5*fn)
print("Cost per obs:", (fp + 5*fn)/len(y_test))
print("Bad recall (TPR):", tp/(tp+fn))
print("Bad precision:", tp/(tp+fp))
print("Approval rate (predict GOOD):", (y_pred==0).mean())

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score, brier_score_loss

# --- Load ---
path = "/german.data"
df = pd.read_csv(path, sep=r"\s+", header=None)

cols = [
    "checking_status", "duration_months", "credit_history", "purpose", "credit_amount",
    "savings_status", "employment_since", "installment_rate", "personal_status_sex",
    "other_debtors", "residence_since", "property", "age", "other_installment_plans",
    "housing", "existing_credits", "job", "num_liable", "telephone", "foreign_worker",
    "label"
]
df.columns = cols

y = (df["label"] == 2).astype(int)   # positivo = BAD
X = df.drop(columns=["label"])

categorical = [
    "checking_status","credit_history","purpose","savings_status","employment_since",
    "personal_status_sex","other_debtors","property","other_installment_plans",
    "housing","job","telephone","foreign_worker"
]
numeric = [c for c in X.columns if c not in categorical]

preprocess = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical),
        ("num", Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler())
        ]), numeric)
    ]
)

def expected_cost(y_true, y_prob, t, fp_cost=1, fn_cost=5):
    y_pred = (y_prob >= t).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return fp_cost * fp + fn_cost * fn

def fold_metrics(y_true, y_prob, t):
    y_pred = (y_prob >= t).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    approval_rate = (y_pred == 0).mean()  # approvo se predico GOOD
    bad_recall = tp / (tp + fn) if (tp + fn) else np.nan
    bad_precision = tp / (tp + fp) if (tp + fp) else np.nan
    bad_rate_approved = fn / (tn + fn) if (tn + fn) else np.nan
    return {
        "tn": tn, "fp": fp, "fn": fn, "tp": tp,
        "approval_rate": approval_rate,
        "bad_recall": bad_recall,
        "bad_precision": bad_precision,
        "bad_rate_approved": bad_rate_approved
    }

# --- Holdout test (mai toccato per tuning soglia) ---
X_tr, X_te, y_tr, y_te = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# --- Model ---
base_model = Pipeline(steps=[
    ("prep", preprocess),
    ("clf", LogisticRegression(max_iter=500, class_weight="balanced"))
])

# --- CV on train (80%) ---
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
ts = np.linspace(0.01, 0.99, 99)

rows = []
thresholds = []

for fold, (idx_tr, idx_va) in enumerate(skf.split(X_tr, y_tr), 1):
    X_train_fold, y_train_fold = X_tr.iloc[idx_tr], y_tr.iloc[idx_tr]
    X_valid_fold, y_valid_fold = X_tr.iloc[idx_va], y_tr.iloc[idx_va]

    base_model.fit(X_train_fold, y_train_fold)
    p_va = base_model.predict_proba(X_valid_fold)[:, 1]

    costs = np.array([expected_cost(y_valid_fold, p_va, t) for t in ts])
    t_star = ts[costs.argmin()]
    thresholds.append(t_star)

    m = fold_metrics(y_valid_fold, p_va, t_star)
    m.update({
        "fold": fold,
        "t_star": t_star,
        "cost": costs.min(),
        "cost_per_obs": costs.min() / len(y_valid_fold),
        "roc_auc": roc_auc_score(y_valid_fold, p_va),
        "pr_auc": average_precision_score(y_valid_fold, p_va),
        "brier": brier_score_loss(y_valid_fold, p_va)
    })
    rows.append(m)

cv_res = pd.DataFrame(rows)

print("\nCV results (train 80%):")
print(cv_res[["fold","t_star","cost","cost_per_obs","approval_rate","bad_rate_approved","bad_recall","bad_precision","roc_auc","pr_auc","brier"]])

print("\nStability (mean ± std):")
for col in ["t_star","cost","cost_per_obs","approval_rate","bad_rate_approved","bad_recall","bad_precision","roc_auc","pr_auc","brier"]:
    print(f"{col}: {cv_res[col].mean():.4f} ± {cv_res[col].std(ddof=1):.4f}")

# --- Final fit on 80% and evaluation on untouched test 20% ---
t_final = float(np.median(thresholds))  # robusto (puoi anche usare mean)
base_model.fit(X_tr, y_tr)
p_te = base_model.predict_proba(X_te)[:, 1]

test_cost = expected_cost(y_te, p_te, t_final)
test_m = fold_metrics(y_te, p_te, t_final)

print("\nFINAL (holdout test 20%):")
print("t_final (median CV):", t_final)
print("ROC-AUC:", roc_auc_score(y_te, p_te))
print("PR-AUC:", average_precision_score(y_te, p_te))
print("Brier:", brier_score_loss(y_te, p_te))
print("Cost:", test_cost, "Cost/obs:", test_cost/len(y_te))
print("TN FP FN TP:", test_m["tn"], test_m["fp"], test_m["fn"], test_m["tp"])
print("Approval rate:", test_m["approval_rate"])
print("Bad rate among approved:", test_m["bad_rate_approved"])
print("Bad recall:", test_m["bad_recall"])
print("Bad precision:", test_m["bad_precision"])

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import (
    confusion_matrix, roc_auc_score, average_precision_score, brier_score_loss
)

# =========================
# 1) Load + schema colonne
# =========================
path = "/german.data"
df = pd.read_csv(path, sep=r"\s+", header=None)

cols = [
    "checking_status", "duration_months", "credit_history", "purpose", "credit_amount",
    "savings_status", "employment_since", "installment_rate", "personal_status_sex",
    "other_debtors", "residence_since", "property", "age", "other_installment_plans",
    "housing", "existing_credits", "job", "num_liable", "telephone", "foreign_worker",
    "label"
]
df.columns = cols

# Target positivo = BAD (label 2)
y = (df["label"] == 2).astype(int)
X = df.drop(columns=["label"])

categorical = [
    "checking_status","credit_history","purpose","savings_status","employment_since",
    "personal_status_sex","other_debtors","property","other_installment_plans",
    "housing","job","telephone","foreign_worker"
]
numeric = [c for c in X.columns if c not in categorical]

preprocess = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical),
        ("num", Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler())
        ]), numeric)
    ]
)

# =========================
# 2) Utility metriche
# =========================
FP_COST = 1
FN_COST = 5

def expected_cost(y_true, y_prob, t, fp_cost=FP_COST, fn_cost=FN_COST):
    y_pred = (y_prob >= t).astype(int)  # 1 = BAD (rifiuta)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return fp_cost * fp + fn_cost * fn

def policy_metrics(y_true, y_prob, t):
    y_pred = (y_prob >= t).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

    approval_rate = (y_pred == 0).mean()                   # approvati = pred GOOD
    bad_rate_approved = fn / (tn + fn) if (tn + fn) else np.nan
    bad_recall = tp / (tp + fn) if (tp + fn) else np.nan   # recall BAD
    bad_precision = tp / (tp + fp) if (tp + fp) else np.nan

    return {
        "tn": tn, "fp": fp, "fn": fn, "tp": tp,
        "approval_rate": approval_rate,
        "bad_rate_approved": bad_rate_approved,
        "bad_recall": bad_recall,
        "bad_precision": bad_precision
    }

# =========================
# 3) Modelli da confrontare
# =========================
def make_estimator(kind: str):
    """
    kind in:
      - "logreg_none"
      - "logreg_balanced"
      - "cal_sigmoid"
      - "cal_isotonic"
    """
    if kind == "logreg_none":
        clf = LogisticRegression(max_iter=800, class_weight=None)
        return Pipeline([("prep", preprocess), ("clf", clf)])

    if kind == "logreg_balanced":
        clf = LogisticRegression(max_iter=800, class_weight="balanced")
        return Pipeline([("prep", preprocess), ("clf", clf)])

    if kind == "cal_sigmoid":
        base = LogisticRegression(max_iter=800, class_weight=None)
        base_pipe = Pipeline([("prep", preprocess), ("clf", base)])
        # calibrazione interna SOLO su training fold
        cal = CalibratedClassifierCV(base_pipe, method="sigmoid", cv=3)
        return cal

    if kind == "cal_isotonic":
        base = LogisticRegression(max_iter=800, class_weight=None)
        base_pipe = Pipeline([("prep", preprocess), ("clf", base)])
        cal = CalibratedClassifierCV(base_pipe, method="isotonic", cv=3)
        return cal

    raise ValueError("Unknown kind")

# =========================
# 4) CV su train80 + test20 finale
# =========================
X_tr, X_te, y_tr, y_te = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
ts = np.linspace(0.01, 0.99, 99)

kinds = ["logreg_none", "logreg_balanced", "cal_sigmoid", "cal_isotonic"]
all_summaries = []

for kind in kinds:
    fold_rows = []
    thresholds = []

    for fold, (idx_tr, idx_va) in enumerate(skf.split(X_tr, y_tr), 1):
        X_train_fold, y_train_fold = X_tr.iloc[idx_tr], y_tr.iloc[idx_tr]
        X_valid_fold, y_valid_fold = X_tr.iloc[idx_va], y_tr.iloc[idx_va]

        est = make_estimator(kind)
        est.fit(X_train_fold, y_train_fold)
        p_va = est.predict_proba(X_valid_fold)[:, 1]

        costs = np.array([expected_cost(y_valid_fold, p_va, t) for t in ts])
        t_star = float(ts[costs.argmin()])
        thresholds.append(t_star)

        m = policy_metrics(y_valid_fold, p_va, t_star)
        m.update({
            "model": kind,
            "fold": fold,
            "t_star": t_star,
            "cost": float(costs.min()),
            "cost_per_obs": float(costs.min() / len(y_valid_fold)),
            "roc_auc": roc_auc_score(y_valid_fold, p_va),
            "pr_auc": average_precision_score(y_valid_fold, p_va),
            "brier": brier_score_loss(y_valid_fold, p_va)
        })
        fold_rows.append(m)

    cv_df = pd.DataFrame(fold_rows)
    t_final = float(np.median(thresholds))

    # Fit finale su train80 e test su holdout20
    est_final = make_estimator(kind)
    est_final.fit(X_tr, y_tr)
    p_te = est_final.predict_proba(X_te)[:, 1]

    test_cost = expected_cost(y_te, p_te, t_final)
    test_pol = policy_metrics(y_te, p_te, t_final)

    summary = {
        "model": kind,
        # CV stability
        "cv_cost_mean": cv_df["cost"].mean(),
        "cv_cost_std": cv_df["cost"].std(ddof=1),
        "cv_cost_per_obs_mean": cv_df["cost_per_obs"].mean(),
        "cv_cost_per_obs_std": cv_df["cost_per_obs"].std(ddof=1),
        "cv_t_star_mean": cv_df["t_star"].mean(),
        "cv_t_star_std": cv_df["t_star"].std(ddof=1),
        "cv_approval_mean": cv_df["approval_rate"].mean(),
        "cv_approval_std": cv_df["approval_rate"].std(ddof=1),
        "cv_bad_rate_approved_mean": cv_df["bad_rate_approved"].mean(),
        "cv_bad_rate_approved_std": cv_df["bad_rate_approved"].std(ddof=1),
        "cv_brier_mean": cv_df["brier"].mean(),
        "cv_brier_std": cv_df["brier"].std(ddof=1),
        "cv_auc_mean": cv_df["roc_auc"].mean(),
        "cv_auc_std": cv_df["roc_auc"].std(ddof=1),
        "cv_prauc_mean": cv_df["pr_auc"].mean(),
        "cv_prauc_std": cv_df["pr_auc"].std(ddof=1),

        # Final threshold & test metrics
        "t_final_median_cv": t_final,
        "test_cost": float(test_cost),
        "test_cost_per_obs": float(test_cost / len(y_te)),
        "test_brier": brier_score_loss(y_te, p_te),
        "test_auc": roc_auc_score(y_te, p_te),
        "test_prauc": average_precision_score(y_te, p_te),
        "test_approval": test_pol["approval_rate"],
        "test_bad_rate_approved": test_pol["bad_rate_approved"],
        "test_bad_recall": test_pol["bad_recall"],
        "test_bad_precision": test_pol["bad_precision"],
        "test_tn": test_pol["tn"], "test_fp": test_pol["fp"],
        "test_fn": test_pol["fn"], "test_tp": test_pol["tp"]
    }

    all_summaries.append(summary)

res = pd.DataFrame(all_summaries)

# Ordina per metrica obiettivo: costo test
res = res.sort_values("test_cost").reset_index(drop=True)

# Mostra tabella compatta “da decisione”
cols_out = [
    "model",
    "test_cost", "test_cost_per_obs",
    "t_final_median_cv",
    "test_approval", "test_bad_rate_approved",
    "test_bad_recall", "test_bad_precision",
    "test_brier", "test_auc", "test_prauc",
    "cv_cost_mean", "cv_cost_std", "cv_t_star_mean", "cv_t_star_std"
]
print(res[cols_out])

from sklearn.model_selection import StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
import numpy as np
import pandas as pd

# Assumo esistano: X_tr, y_tr, preprocess

FP_COST, FN_COST = 1, 5
APPROVAL_MIN = 0.50
ts = np.linspace(0.01, 0.99, 99)

def cost_and_approval(y_true, p, t, fp_cost=FP_COST, fn_cost=FN_COST):
    y_pred = (p >= t).astype(int)  # 1=BAD (rifiuta), 0=GOOD (approva)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    cost = fp_cost*fp + fn_cost*fn
    approval = (y_pred == 0).mean()
    bad_rate_approved = fn / (tn + fn) if (tn + fn) else np.nan
    return cost, approval, bad_rate_approved, (tn, fp, fn, tp)

# Modello winner (no class_weight)
logreg_none = Pipeline([
    ("prep", preprocess),
    ("clf", LogisticRegression(max_iter=800, class_weight=None))
])

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

fold_rows = []
t_stars = []

for fold, (idx_tr, idx_va) in enumerate(skf.split(X_tr, y_tr), 1):
    X_train_fold, y_train_fold = X_tr.iloc[idx_tr], y_tr.iloc[idx_tr]
    X_valid_fold, y_valid_fold = X_tr.iloc[idx_va], y_tr.iloc[idx_va]

    logreg_none.fit(X_train_fold, y_train_fold)
    p_va = logreg_none.predict_proba(X_valid_fold)[:, 1]

    # calcolo costi/approval per tutte le soglie
    grid = []
    for t in ts:
        cost, approval, bad_rate_app, cm = cost_and_approval(y_valid_fold, p_va, t)
        grid.append((t, cost, approval, bad_rate_app))
    grid = pd.DataFrame(grid, columns=["t", "cost", "approval", "bad_rate_approved"])

    # vincolo approval >= 0.50
    feasible = grid[grid["approval"] >= APPROVAL_MIN]
    if feasible.empty:
        # se nessuna soglia soddisfa, prendi quella con approval massimo (fallback)
        best = grid.sort_values(["approval", "cost"], ascending=[False, True]).iloc[0]
    else:
        best = feasible.sort_values("cost", ascending=True).iloc[0]

    t_star = float(best["t"])
    t_stars.append(t_star)

    fold_rows.append({
        "fold": fold,
        "t_star": t_star,
        "cost": float(best["cost"]),
        "approval": float(best["approval"]),
        "bad_rate_approved": float(best["bad_rate_approved"]),
    })

cv_constrained = pd.DataFrame(fold_rows)
t_final = float(np.median(t_stars))

cv_constrained, t_final

from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, confusion_matrix
import numpy as np

# Assumo esistano: X_te, y_te, logreg_none, t_final

logreg_none.fit(X_tr, y_tr)
p_te = logreg_none.predict_proba(X_te)[:, 1]
y_pred = (p_te >= t_final).astype(int)

tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()
test_cost = fp + 5*fn
approval = (y_pred == 0).mean()
bad_rate_approved = fn / (tn + fn) if (tn + fn) else np.nan
bad_recall = tp / (tp + fn) if (tp + fn) else np.nan
bad_precision = tp / (tp + fp) if (tp + fp) else np.nan

print("t_final:", t_final)
print("TN FP FN TP:", tn, fp, fn, tp)
print("Cost:", test_cost, "Cost/obs:", test_cost/len(y_te))
print("Approval:", approval)
print("Bad rate among approved:", bad_rate_approved)
print("Bad recall:", bad_recall)
print("Bad precision:", bad_precision)

print("AUC:", roc_auc_score(y_te, p_te))
print("PR-AUC:", average_precision_score(y_te, p_te))
print("Brier:", brier_score_loss(y_te, p_te))

import numpy as np
import pandas as pd

from sklearn.model_selection import StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score, brier_score_loss
from sklearn.ensemble import RandomForestClassifier

# Se non hai xgboost installato: pip install xgboost
from xgboost import XGBClassifier

FP_COST, FN_COST = 1, 5
APPROVAL_MIN = 0.50
ts = np.linspace(0.01, 0.99, 99)

def cost_and_approval(y_true, p, t, fp_cost=FP_COST, fn_cost=FN_COST):
    y_pred = (p >= t).astype(int)  # 1=BAD (rifiuta), 0=GOOD (approva)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    cost = fp_cost*fp + fn_cost*fn
    approval = (y_pred == 0).mean()
    bad_rate_approved = fn / (tn + fn) if (tn + fn) else np.nan
    bad_recall = tp / (tp + fn) if (tp + fn) else np.nan
    bad_precision = tp / (tp + fp) if (tp + fp) else np.nan
    return cost, approval, bad_rate_approved, bad_recall, bad_precision, (tn, fp, fn, tp)

def select_threshold_constrained(y_true, p, approval_min=APPROVAL_MIN):
    grid = []
    for t in ts:
        cost, approval, bad_rate_app, bad_rec, bad_prec, _ = cost_and_approval(y_true, p, t)
        grid.append((t, cost, approval, bad_rate_app, bad_rec, bad_prec))
    grid = pd.DataFrame(grid, columns=["t","cost","approval","bad_rate_approved","bad_recall","bad_precision"])

    feasible = grid[grid["approval"] >= approval_min]
    if feasible.empty:
        # fallback: massimizza approval e poi minimizza costo
        best = grid.sort_values(["approval","cost"], ascending=[False, True]).iloc[0]
    else:
        best = feasible.sort_values("cost", ascending=True).iloc[0]
    return float(best["t"]), grid

def evaluate_model_constrained(name, model, X_tr, y_tr, X_te, y_te):
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    fold_rows = []
    t_stars = []

    for fold, (idx_tr, idx_va) in enumerate(skf.split(X_tr, y_tr), 1):
        X_train_fold, y_train_fold = X_tr.iloc[idx_tr], y_tr.iloc[idx_tr]
        X_valid_fold, y_valid_fold = X_tr.iloc[idx_va], y_tr.iloc[idx_va]

        model.fit(X_train_fold, y_train_fold)
        p_va = model.predict_proba(X_valid_fold)[:, 1]

        t_star, _ = select_threshold_constrained(y_valid_fold, p_va, APPROVAL_MIN)
        t_stars.append(t_star)

        # metriche su valid fold con t_star
        cost, approval, bad_rate_app, bad_rec, bad_prec, cm = cost_and_approval(y_valid_fold, p_va, t_star)
        tn, fp, fn, tp = cm
        fold_rows.append({
            "model": name, "fold": fold, "t_star": t_star,
            "cost": cost, "cost_per_obs": cost/len(y_valid_fold),
            "approval": approval, "bad_rate_approved": bad_rate_app,
            "bad_recall": bad_rec, "bad_precision": bad_prec,
            "roc_auc": roc_auc_score(y_valid_fold, p_va),
            "pr_auc": average_precision_score(y_valid_fold, p_va),
            "brier": brier_score_loss(y_valid_fold, p_va),
            "tn": tn, "fp": fp, "fn": fn, "tp": tp
        })

    cv_df = pd.DataFrame(fold_rows)
    t_final = float(np.median(t_stars))

    # Fit finale su train80 e test su holdout20
    model.fit(X_tr, y_tr)
    p_te = model.predict_proba(X_te)[:, 1]

    cost, approval, bad_rate_app, bad_rec, bad_prec, cm = cost_and_approval(y_te, p_te, t_final)
    tn, fp, fn, tp = cm

    test_res = {
        "model": name,
        "t_final": t_final,
        "test_cost": cost, "test_cost_per_obs": cost/len(y_te),
        "test_approval": approval,
        "test_bad_rate_approved": bad_rate_app,
        "test_bad_recall": bad_rec,
        "test_bad_precision": bad_prec,
        "test_auc": roc_auc_score(y_te, p_te),
        "test_prauc": average_precision_score(y_te, p_te),
        "test_brier": brier_score_loss(y_te, p_te),
        "TN": tn, "FP": fp, "FN": fn, "TP": tp,
        "cv_cost_mean": cv_df["cost"].mean(),
        "cv_cost_std": cv_df["cost"].std(ddof=1),
        "cv_t_mean": cv_df["t_star"].mean(),
        "cv_t_std": cv_df["t_star"].std(ddof=1),
        "cv_approval_mean": cv_df["approval"].mean(),
        "cv_approval_std": cv_df["approval"].std(ddof=1),
    }

    return cv_df, pd.Series(test_res)

# ---------------------------
# MODELLI
# ---------------------------

# Random Forest (setup conservativo per stabilità)
rf = Pipeline([
    ("prep", preprocess),
    ("clf", RandomForestClassifier(
        n_estimators=600,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    ))
])

# XGBoost (setup conservativo, niente tuning aggressivo ora)
xgb = Pipeline([
    ("prep", preprocess),
    ("clf", XGBClassifier(
        n_estimators=600,
        max_depth=3,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_lambda=1.0,
        random_state=42,
        eval_metric="logloss",
        n_jobs=-1
    ))
])

# ---------------------------
# ESECUZIONE
# ---------------------------
cv_rf, test_rf = evaluate_model_constrained("RandomForest", rf, X_tr, y_tr, X_te, y_te)
cv_xgb, test_xgb = evaluate_model_constrained("XGBoost", xgb, X_tr, y_tr, X_te, y_te)

summary = pd.DataFrame([test_rf, test_xgb]).sort_values("test_cost")
summary[[
    "model","test_cost","test_cost_per_obs","t_final",
    "test_approval","test_bad_rate_approved","test_bad_recall","test_bad_precision",
    "test_auc","test_prauc","test_brier",
    "cv_cost_mean","cv_cost_std","cv_t_mean","cv_t_std"
]]

import numpy as np
import pandas as pd

# Assumo esistano: logreg_none, X_tr, y_tr
# Se non è già allenato, allenalo:
logreg_none.fit(X_tr, y_tr)

# Recupero nomi feature dopo OneHot + numeriche
prep = logreg_none.named_steps["prep"]
clf = logreg_none.named_steps["clf"]

feature_names = prep.get_feature_names_out()
coefs = clf.coef_.ravel()  # coeff per classe positiva = BAD

feat_imp = (
    pd.DataFrame({"feature": feature_names, "coef": coefs})
      .assign(abs_coef=lambda d: d["coef"].abs())
      .sort_values("coef", ascending=False)
      .reset_index(drop=True)
)

top_risk = feat_imp.head(10)               # coef positivi: aumenta rischio BAD
top_protect = feat_imp.tail(10).iloc[::-1] # coef negativi: riduce rischio BAD

print("\nTop 10 fattori che AUMENTANO la probabilità di BAD (rischio):")
print(top_risk[["feature","coef"]].to_string(index=False))

print("\nTop 10 fattori che DIMINUISCONO la probabilità di BAD (protezione):")
print(top_protect[["feature","coef"]].to_string(index=False))

top_risk_or = top_risk.assign(odds_ratio=np.exp(top_risk["coef"]))
top_protect_or = top_protect.assign(odds_ratio=np.exp(top_protect["coef"]))

print("\nTop 10 rischio con Odds Ratio:")
print(top_risk_or[["feature","coef","odds_ratio"]].to_string(index=False))

print("\nTop 10 protezione con Odds Ratio:")
print(top_protect_or[["feature","coef","odds_ratio"]].to_string(index=False))

import re
from pathlib import Path

doc_path = Path("/german.data")
text = doc_path.read_text(errors="ignore")

# Estrae mapping del tipo: A11 : ... , A12 : ...
# NB: il .doc qui è in realtà testo (nel dataset UCI spesso è un file plain text).
code_map = {}
for m in re.finditer(r"\b(A\d{2,3})\s*:\s*(.+)", text):
    code = m.group(1).strip()
    desc = m.group(2).strip()
    # pulizia minima
    desc = re.sub(r"\s+", " ", desc)
    code_map[code] = desc

def humanize(feature_name: str):
    # feature_name esempio: "cat__purpose_A46" oppure "cat__checking_status_A14"
    parts = feature_name.split("_")
    code = parts[-1]  # A46
    var = feature_name.split("__")[1].rsplit("_", 1)[0]  # purpose
    return f"{var} = {code_map.get(code, code)}"

# Prendo le tue tabelle già calcolate: top_risk_or e top_protect_or
top_risk_readable = top_risk_or.copy()
top_risk_readable["meaning"] = top_risk_readable["feature"].apply(humanize)

top_protect_readable = top_protect_or.copy()
top_protect_readable["meaning"] = top_protect_readable["feature"].apply(humanize)

print("\nTOP rischio (in chiaro):")
print(top_risk_readable[["meaning","coef","odds_ratio"]].to_string(index=False))

print("\nTOP protezione (in chiaro):")
print(top_protect_readable[["meaning","coef","odds_ratio"]].to_string(index=False))

# Assumo esistano: X_tr, X_te, y_tr, y_te, preprocess, APPROVAL_MIN, ts
# e le funzioni: cost_and_approval, select_threshold_constrained, evaluate_model_constrained
# Se non hai più le funzioni in memoria dimmelo e te le ripasso.

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer

SENSITIVE = ["foreign_worker", "personal_status_sex"]

def drop_from_preprocess(preprocess, drop_cols):
    # ricostruisce il ColumnTransformer rimuovendo le colonne indicate
    new_transformers = []
    for name, transformer, cols in preprocess.transformers_:
        if isinstance(cols, list):
            cols_new = [c for c in cols if c not in drop_cols]
        else:
            cols_new = cols
        new_transformers.append((name, transformer, cols_new))
    return ColumnTransformer(new_transformers)

# preprocess senza sensibili
preprocess_nosens = drop_from_preprocess(preprocess, SENSITIVE)

logreg_all = Pipeline([("prep", preprocess), ("clf", LogisticRegression(max_iter=800, class_weight=None))])
logreg_nosens = Pipeline([("prep", preprocess_nosens), ("clf", LogisticRegression(max_iter=800, class_weight=None))])

cv_all, test_all = evaluate_model_constrained("LogReg_ALL", logreg_all, X_tr, y_tr, X_te, y_te)
cv_nos, test_nos = evaluate_model_constrained("LogReg_NO_SENS", logreg_nosens, X_tr, y_tr, X_te, y_te)

pd.DataFrame([test_all, test_nos])[[
    "model","test_cost","test_cost_per_obs","t_final",
    "test_approval","test_bad_rate_approved","test_bad_recall","test_bad_precision",
    "test_brier","test_auc","test_prauc",
    "cv_cost_mean","cv_cost_std","cv_t_mean","cv_t_std"
]].sort_values("test_cost")

# Assumo esistano X (o almeno X_tr/X_te originali) con le colonne categoriali originali
# e la lista "categorical" che avevi definito.

# Mostra per ogni variabile categorica la distribuzione delle categorie (top e rare)
for col in categorical:
    counts = X_tr[col].value_counts(dropna=False)
    rare = counts[counts < 10]  # soglia semplice (10)
    print(f"\n{col}: #categorie={counts.shape[0]}")
    print("Top 5:", counts.head(5).to_dict())
    if len(rare) > 0:
        print("Rare (<10):", rare.to_dict())

# Assumo esistano: logreg_none (o il modello finale scelto), X_tr, y_tr, X_te, y_te
# e la funzione cost_and_approval

model_final = logreg_none  # oppure logreg_nosens se vince
model_final.fit(X_tr, y_tr)
p_te = model_final.predict_proba(X_te)[:, 1]

thresholds = [0.10, 0.15, 0.20, 0.24, 0.30, 0.35, 0.40]
rows = []
for t in thresholds:
    cost, approval, bad_rate_app, bad_rec, bad_prec, cm = cost_and_approval(y_te, p_te, t)
    tn, fp, fn, tp = cm
    rows.append({
        "t": t,
        "cost": cost,
        "approval": approval,
        "bad_rate_approved": bad_rate_app,
        "bad_recall": bad_rec,
        "bad_precision": bad_prec,
        "FP": fp, "FN": fn
    })

pd.DataFrame(rows)

# Assumo esistano: logreg_nosens, X_tr, y_tr, X_te, y_te, cost_and_approval

logreg_nosens.fit(X_tr, y_tr)
p_te = logreg_nosens.predict_proba(X_te)[:, 1]

thresholds = [0.10, 0.15, 0.20, 0.24, 0.25, 0.30, 0.35, 0.40]
rows = []
for t in thresholds:
    cost, approval, bad_rate_app, bad_rec, bad_prec, cm = cost_and_approval(y_te, p_te, t)
    tn, fp, fn, tp = cm
    rows.append({
        "t": t, "cost": cost, "approval": approval,
        "bad_rate_approved": bad_rate_app, "bad_recall": bad_rec,
        "bad_precision": bad_prec, "FP": fp, "FN": fn
    })

pd.DataFrame(rows)

# Assumo esistano: X_tr, y_tr, logreg_nosens, APPROVAL_MIN, ts
# e le funzioni: cost_and_approval, select_threshold_constrained
from sklearn.model_selection import StratifiedKFold
import numpy as np
import pandas as pd

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

fold_rows = []
t_stars = []

for fold, (idx_tr, idx_va) in enumerate(skf.split(X_tr, y_tr), 1):
    X_train_fold, y_train_fold = X_tr.iloc[idx_tr], y_tr.iloc[idx_tr]
    X_valid_fold, y_valid_fold = X_tr.iloc[idx_va], y_tr.iloc[idx_va]

    logreg_nosens.fit(X_train_fold, y_train_fold)
    p_va = logreg_nosens.predict_proba(X_valid_fold)[:, 1]

    t_star, _ = select_threshold_constrained(y_valid_fold, p_va, APPROVAL_MIN)
    t_stars.append(t_star)

    cost, approval, bad_rate_app, bad_rec, bad_prec, cm = cost_and_approval(y_valid_fold, p_va, t_star)
    tn, fp, fn, tp = cm
    fold_rows.append({
        "fold": fold, "t_star": t_star, "cost": cost,
        "approval": approval, "bad_rate_approved": bad_rate_app,
        "bad_recall": bad_rec, "bad_precision": bad_prec,
        "FP": fp, "FN": fn
    })

cv_nosens_constrained = pd.DataFrame(fold_rows)
t_final_cv = float(np.median(t_stars))

cv_nosens_constrained, t_final_cv

logreg_nosens.fit(X_tr, y_tr)
p_te = logreg_nosens.predict_proba(X_te)[:, 1]

cost, approval, bad_rate_app, bad_rec, bad_prec, cm = cost_and_approval(y_te, p_te, t_final_cv)
tn, fp, fn, tp = cm

print("t_final_cv:", t_final_cv)
print("TN FP FN TP:", tn, fp, fn, tp)
print("Cost:", cost, "Cost/obs:", cost/len(y_te))
print("Approval:", approval)
print("Bad rate among approved:", bad_rate_app)
print("Bad recall:", bad_rec)
print("Bad precision:", bad_prec)

import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# 1) Definisci i 2 modelli LogReg (ALL e NO_SENS)
logreg_all = Pipeline([("prep", preprocess), ("clf", LogisticRegression(max_iter=800, class_weight=None))])
logreg_nosens = Pipeline([("prep", preprocess_nosens), ("clf", LogisticRegression(max_iter=800, class_weight=None))])

# 2) Valuta tutti con la stessa funzione vincolata
_, test_all = evaluate_model_constrained("LogReg_ALL", logreg_all, X_tr, y_tr, X_te, y_te)
_, test_nos = evaluate_model_constrained("LogReg_NO_SENS", logreg_nosens, X_tr, y_tr, X_te, y_te)

_, test_rf = evaluate_model_constrained("RandomForest", rf, X_tr, y_tr, X_te, y_te)
_, test_xgb = evaluate_model_constrained("XGBoost", xgb, X_tr, y_tr, X_te, y_te)

final = pd.DataFrame([test_all, test_nos, test_rf, test_xgb])

# 3) Tabella compatta “da report”
final_report = final[[
    "model",
    "test_cost","test_cost_per_obs","t_final",
    "test_approval","test_bad_rate_approved",
    "test_bad_recall","test_bad_precision",
    "TN","FP","FN","TP",
    "test_auc","test_prauc","test_brier",
    "cv_cost_mean","cv_cost_std","cv_t_mean","cv_t_std"
]].sort_values("test_cost")

final_report

import numpy as np
import pandas as pd
import re
from pathlib import Path

# Fit modello finale
logreg_nosens.fit(X_tr, y_tr)

prep = logreg_nosens.named_steps["prep"]
clf  = logreg_nosens.named_steps["clf"]

feature_names = prep.get_feature_names_out()
coefs = clf.coef_.ravel()

imp = (pd.DataFrame({"feature": feature_names, "coef": coefs})
         .assign(odds_ratio=lambda d: np.exp(d["coef"]))
         .sort_values("coef", ascending=False)
         .reset_index(drop=True))

top_risk = imp.head(15)               # aumenta rischio BAD
top_prot = imp.tail(15).iloc[::-1]    # diminuisce rischio BAD

doc_path = Path("/german.data")
text = doc_path.read_text(errors="ignore")

code_map = {}
for m in re.finditer(r"\b(A\d{2,3})\s*:\s*(.+)", text):
    code = m.group(1).strip()
    desc = re.sub(r"\s+", " ", m.group(2).strip())
    code_map[code] = desc

def parse_var_and_code(f):
    # Esempi:
    # "cat__purpose_A40" -> var="purpose", code="A40"
    # "num__age" -> var="age", code=None
    if f.startswith("cat__"):
        right = f.split("__", 1)[1]
        var, code = right.rsplit("_", 1)
        return var, code
    if f.startswith("num__"):
        return f.split("__", 1)[1], None
    return f, None

def humanize_feature(f):
    var, code = parse_var_and_code(f)
    if code is None:
        return f"{var} (numerica)"
    return f"{var} = {code_map.get(code, code)}"

# Frequenze categorie sul train per le variabili categoriche presenti in X_tr
# (quelle ancora presenti nel modello NO_SENS)
cat_cols_present = [c for c in X_tr.columns if X_tr[c].dtype == "object"]

freq_maps = {c: X_tr[c].value_counts() for c in cat_cols_present}
RARE_N = 10

def is_rare(f):
    var, code = parse_var_and_code(f)
    if code is None:
        return False
    if var not in freq_maps:
        return False
    return int(freq_maps[var].get(code, 0)) < RARE_N

def add_readability(df):
    out = df.copy()
    out["meaning"] = out["feature"].apply(humanize_feature)
    out["rare_lt10"] = out["feature"].apply(is_rare)
    return out[["meaning","coef","odds_ratio","rare_lt10"]]

risk_readable = add_readability(top_risk)
prot_readable = add_readability(top_prot)

print("\nTOP driver che AUMENTANO rischio (BAD) – modello NO_SENS:")
print(risk_readable.to_string(index=False))

print("\nTOP driver che DIMINUISCONO rischio (protezione) – modello NO_SENS:")
print(prot_readable.to_string(index=False))

import pandas as pd
import json
from pathlib import Path

# === Percorso sul tuo Mac ===
out_dir = Path("/Users/gengis/Desktop/CREDIT-SCORING ")
out_dir.mkdir(parents=True, exist_ok=True)

# 1) Confronto modelli
final_report.to_csv(out_dir / "RISULTATI_MODELLI.csv", index=False)

# 2) Risultato finale modello vincente (NO_SENS)
logreg_nosens.fit(X_tr, y_tr)
p_te = logreg_nosens.predict_proba(X_te)[:, 1]

t_final = float(t_final_cv)  # 0.25 nel tuo caso
cost, approval, bad_rate_app, bad_rec, bad_prec, cm = cost_and_approval(y_te, p_te, t_final)
tn, fp, fn, tp = cm

risultato_finale = {
    "modello": "LogReg_NO_SENS",
    "soglia": t_final,
    "costi_errori": {"FP": 1, "FN": 5},
    "test_holdout": {
        "N": int(len(y_te)),
        "TN": int(tn), "FP": int(fp), "FN": int(fn), "TP": int(tp),
        "costo_totale": float(cost),
        "costo_medio_per_cliente": float(cost / len(y_te)),
        "tasso_approvazione": float(approval),
        "rischio_tra_approvati": float(bad_rate_app),
        "recall_bad": float(bad_rec),
        "precision_bad": float(bad_prec)
    }
}

with open(out_dir / "RISULTATO_FINALE_MODELLO.json", "w") as f:
    json.dump(risultato_finale, f, indent=2)

# 3) Fattori importanti (tolgo categorie rare)
drivers = pd.concat([
    risk_readable.assign(direzione="aumenta_rischio"),
    prot_readable.assign(direzione="riduce_rischio")
], ignore_index=True)

drivers_clean = drivers[drivers["rare_lt10"] == False].copy()
drivers_clean.to_csv(out_dir / "FATTORI_PIU_IMPORTANTI.csv", index=False)

print("✅ File salvati in:", out_dir)
for name in ["RISULTATI_MODELLI.csv", "RISULTATO_FINALE_MODELLO.json", "FATTORI_PIU_IMPORTANTI.csv"]:
    print(" -", (out_dir / name))

# cv_nosens_constrained: tabella fold del modello NO_SENS vincolato
cv_nosens_constrained.agg({
    "cost": ["mean","std","min","max"],
    "approval": ["mean","std","min","max"],
    "bad_rate_approved": ["mean","std","min","max"],
    "bad_recall": ["mean","std","min","max"],
    "bad_precision": ["mean","std","min","max"],
    "t_star": ["mean","std","min","max"],
})

import numpy as np
import matplotlib.pyplot as plt

logreg_nosens.fit(X_tr, y_tr)
p_te = logreg_nosens.predict_proba(X_te)[:, 1]

ts = np.linspace(0.01, 0.99, 99)
costs, approvals = [], []

for t in ts:
    cost, approval, *_ = cost_and_approval(y_te, p_te, t)
    costs.append(cost)
    approvals.append(approval)

t_star = float(t_final_cv)

plt.figure()
plt.plot(ts, costs)
plt.axvline(t_star, linestyle="--")
plt.title("Costo atteso vs soglia (test holdout)")
plt.xlabel("Soglia (probabilità BAD)")
plt.ylabel("Costo (FP + 5·FN)")
plt.show()

plt.figure()
plt.plot(ts, approvals)
plt.axhline(0.50, linestyle="--")
plt.axvline(t_star, linestyle="--")
plt.title("Approval rate vs soglia (test holdout)")
plt.xlabel("Soglia (probabilità BAD)")
plt.ylabel("Approval rate (pred GOOD)")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Assumo che tu abbia già p_te, y_te e t_final_cv
t_star = float(t_final_cv)
y_pred = (p_te >= t_star).astype(int)

tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()
cm = np.array([[tn, fp],
               [fn, tp]])

# Normalizzazione per riga (percentuali per classe vera)
cm_row = cm / cm.sum(axis=1, keepdims=True)

fig, ax = plt.subplots(figsize=(6, 5))

# Colori: "Blues" è molto leggibile e coerente (sequenziale)
im = ax.imshow(cm, cmap="Blues")

# Colorbar
cbar = fig.colorbar(im, ax=ax)
cbar.set_label("Conteggi", rotation=90)

# Assi
ax.set_title(f"Confusion Matrix (soglia t = {t_star:.2f})")
ax.set_xticks([0, 1], labels=["Pred GOOD", "Pred BAD"])
ax.set_yticks([0, 1], labels=["True GOOD", "True BAD"])
ax.set_xlabel("Predizione")
ax.set_ylabel("Verità")

# Testo dentro celle (conteggio + % per riga)
# Soglia per scegliere colore testo (bianco su celle scure)
threshold = cm.max() * 0.5

for i in range(2):
    for j in range(2):
        count = cm[i, j]
        perc = cm_row[i, j] * 100
        txt_color = "white" if count > threshold else "black"
        ax.text(j, i, f"{count}\n({perc:.1f}%)",
                ha="center", va="center", color=txt_color, fontsize=12)

plt.tight_layout()
plt.show()

import re
import pandas as pd
from pathlib import Path


doc_path = Path("/german.doc")
text = doc_path.read_text(errors="ignore")

# Parser robusto: prende mapping sia con ":" sia senza
code_map = {}

# Caso 1: "A11 : ... "
for m in re.finditer(r"(?m)^\s*(A\d{2,3})\s*:\s*(.+?)\s*$", text):
    code_map[m.group(1)] = m.group(2).strip()

# Caso 2: "A11 ... " (senza due punti)
for m in re.finditer(r"(?m)^\s*(A\d{2,3})\s+(.+?)\s*$", text):
    code = m.group(1)
    desc = m.group(2).strip()
    # Evita di sovrascrivere se già preso dal caso con ":"
    code_map.setdefault(code, desc)

# Driver già esportati (puliti dalle rare)
drivers = pd.read_csv("/Users/gengis/Desktop/CREDIT-SCORING /FATTORI_PIU_IMPORTANTI.csv")

# (se nel CSV hai ancora rare, filtrale qui)
if "rare_lt10" in drivers.columns:
    drivers = drivers[drivers["rare_lt10"] == False].copy()

def pretty_var(var: str) -> str:
    # Nomi più leggibili
    mapping = {
        "checking_status": "Checking account",
        "savings_status": "Savings",
        "credit_history": "Credit history",
        "purpose": "Purpose",
        "property": "Property",
        "housing": "Housing",
        "employment_since": "Employment length",
        "other_debtors": "Other debtors",
        "other_installment_plans": "Other installment plans",
        "duration_months": "Duration (months)",
        "installment_rate": "Installment rate",
        "credit_amount": "Credit amount",
        "age": "Age",
        "job": "Job",
        "telephone": "Telephone",
        "existing_credits": "Existing credits",
        "residence_since": "Residence since",
        "num_liable": "Number liable",
    }
    return mapping.get(var, var.replace("_", " ").title())

def humanize_meaning(s: str) -> str:
    # s esempio: "purpose = A46" oppure "duration_months (numerica)"
    s = str(s)

    # Numeriche
    if "(numerica)" in s or "(numeric)" in s:
        var = s.split("(")[0].strip()
        return pretty_var(var)

    # Categoriche tipo "var = Axx"
    m = re.match(r"^\s*([a-zA-Z_]+)\s*=\s*(A\d{2,3})\s*$", s)
    if m:
        var, code = m.group(1), m.group(2)
        desc = code_map.get(code, code)
        # Etichetta finale: "Var: descrizione (Axx)"
        return f"{pretty_var(var)}: {desc} ({code})"

    return s  # fallback

drivers["label_human"] = drivers["meaning"].apply(humanize_meaning)

# Controllo rapido: quante descrizioni sono state risolte davvero?
resolved = drivers["label_human"].str.contains(r"\(A\d{2,3}\)").mean()
print(f"Driver con descrizione risolta (contengono '(Axx)'): {resolved:.1%}")

drivers.head(10)

import matplotlib.pyplot as plt

# Seleziono top 8 rischio e top 8 protezione
risk = drivers[drivers["direzione"] == "aumenta_rischio"].copy()
prot = drivers[drivers["direzione"] == "riduce_rischio"].copy()

risk_top = risk.sort_values("coef", ascending=False).head(8)
prot_top = prot.sort_values("coef", ascending=True).head(8)

plot_df = pd.concat([prot_top, risk_top], ignore_index=True)

plt.figure(figsize=(10, 6))
plt.barh(plot_df["label_human"], plot_df["coef"])
plt.title("Driver principali (LogReg_NO_SENS) – etichette leggibili")
plt.xlabel("Coefficiente (positivo = più rischio BAD)")
plt.ylabel("")
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix

APPROVAL_MIN = 0.50
ts = np.linspace(0.01, 0.99, 99)

def cost_metrics(y_true, p, t, fp_cost, fn_cost):
    y_pred = (p >= t).astype(int)  # 1=BAD (reject), 0=GOOD (approve)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    cost = fp_cost*fp + fn_cost*fn
    approval = (y_pred == 0).mean()
    bad_rate_approved = fn / (tn + fn) if (tn + fn) else np.nan
    bad_recall = tp / (tp + fn) if (tp + fn) else np.nan
    bad_precision = tp / (tp + fp) if (tp + fp) else np.nan
    return cost, approval, bad_rate_approved, bad_recall, bad_precision, (tn, fp, fn, tp)

def select_t_constrained(y_true, p, fp_cost, fn_cost, approval_min=APPROVAL_MIN):
    best = None
    for t in ts:
        cost, approval, bad_rate_app, bad_rec, bad_prec, _ = cost_metrics(y_true, p, t, fp_cost, fn_cost)
        if approval >= approval_min:
            if (best is None) or (cost < best["cost"]):
                best = {"t": t, "cost": cost, "approval": approval,
                        "bad_rate_approved": bad_rate_app, "bad_recall": bad_rec, "bad_precision": bad_prec}
    if best is None:
        # fallback: massimizza approval e poi minimizza costo
        best = {"t": 0.99, "cost": np.inf, "approval": -np.inf}
        for t in ts:
            cost, approval, bad_rate_app, bad_rec, bad_prec, _ = cost_metrics(y_true, p, t, fp_cost, fn_cost)
            cand = (approval, -cost)
            best_cand = (best["approval"], -best["cost"])
            if cand > best_cand:
                best = {"t": t, "cost": cost, "approval": approval,
                        "bad_rate_approved": bad_rate_app, "bad_recall": bad_rec, "bad_precision": bad_prec}
    return float(best["t"])

def cv_threshold(logreg_model, X_tr, y_tr, fp_cost, fn_cost):
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    t_stars = []
    for idx_tr, idx_va in skf.split(X_tr, y_tr):
        X_train_fold, y_train_fold = X_tr.iloc[idx_tr], y_tr.iloc[idx_tr]
        X_valid_fold, y_valid_fold = X_tr.iloc[idx_va], y_tr.iloc[idx_va]
        logreg_model.fit(X_train_fold, y_train_fold)
        p_va = logreg_model.predict_proba(X_valid_fold)[:, 1]
        t_stars.append(select_t_constrained(y_valid_fold, p_va, fp_cost, fn_cost, APPROVAL_MIN))
    return float(np.median(t_stars))

def evaluate_cost_ratio(fp_cost, fn_cost):
    # soglia da CV
    t_final = cv_threshold(logreg_nosens, X_tr, y_tr, fp_cost, fn_cost)

    # test
    logreg_nosens.fit(X_tr, y_tr)
    p_te = logreg_nosens.predict_proba(X_te)[:, 1]
    cost, approval, bad_rate_app, bad_rec, bad_prec, cm = cost_metrics(y_te, p_te, t_final, fp_cost, fn_cost)
    tn, fp, fn, tp = cm
    return {
        "FN:FP": f"{fn_cost}:{fp_cost}",
        "t_final_cv": t_final,
        "test_cost": cost,
        "test_cost_per_obs": cost/len(y_te),
        "approval": approval,
        "bad_rate_approved": bad_rate_app,
        "bad_recall": bad_rec,
        "bad_precision": bad_prec,
        "TN": tn, "FP": fp, "FN": fn, "TP": tp
    }

rows = []
for fn_cost in [3, 5, 10]:
    rows.append(evaluate_cost_ratio(fp_cost=1, fn_cost=fn_cost))

sens_table = pd.DataFrame(rows).sort_values("test_cost")
sens_table

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix

ts = np.linspace(0.01, 0.99, 99)

def cost_metrics(y_true, p, t, fp_cost, fn_cost):
    y_pred = (p >= t).astype(int)  # 1=BAD, 0=GOOD
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    cost = fp_cost*fp + fn_cost*fn
    approval = (y_pred == 0).mean()
    bad_rate_approved = fn / (tn + fn) if (tn + fn) else np.nan
    bad_recall = tp / (tp + fn) if (tp + fn) else np.nan
    bad_precision = tp / (tp + fp) if (tp + fp) else np.nan
    return cost, approval, bad_rate_approved, bad_recall, bad_precision, (tn, fp, fn, tp)

def select_t_constrained(y_true, p, fp_cost, fn_cost, approval_min):
    best = None
    for t in ts:
        cost, approval, bad_rate_app, bad_rec, bad_prec, _ = cost_metrics(y_true, p, t, fp_cost, fn_cost)
        if approval >= approval_min:
            if (best is None) or (cost < best["cost"]):
                best = {"t": t, "cost": cost, "approval": approval}
    if best is None:
        # fallback: massimizza approval, poi minimizza costo
        best = {"t": 0.99, "approval": -1, "cost": np.inf}
        for t in ts:
            cost, approval, *_ = cost_metrics(y_true, p, t, fp_cost, fn_cost)
            if (approval > best["approval"]) or (approval == best["approval"] and cost < best["cost"]):
                best = {"t": t, "cost": cost, "approval": approval}
    return float(best["t"])

def cv_threshold(model, X_tr, y_tr, fp_cost, fn_cost, approval_min):
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    t_stars = []
    for idx_tr, idx_va in skf.split(X_tr, y_tr):
        X_train_fold, y_train_fold = X_tr.iloc[idx_tr], y_tr.iloc[idx_tr]
        X_valid_fold, y_valid_fold = X_tr.iloc[idx_va], y_tr.iloc[idx_va]
        model.fit(X_train_fold, y_train_fold)
        p_va = model.predict_proba(X_valid_fold)[:, 1]
        t_stars.append(select_t_constrained(y_valid_fold, p_va, fp_cost, fn_cost, approval_min))
    return float(np.median(t_stars))

def evaluate_setting(fp_cost, fn_cost, approval_min):
    t_final = cv_threshold(logreg_nosens, X_tr, y_tr, fp_cost, fn_cost, approval_min)
    logreg_nosens.fit(X_tr, y_tr)
    p_te = logreg_nosens.predict_proba(X_te)[:, 1]
    cost, approval, bad_rate_app, bad_rec, bad_prec, cm = cost_metrics(y_te, p_te, t_final, fp_cost, fn_cost)
    tn, fp, fn, tp = cm
    return {
        "approval_min": approval_min,
        "FN:FP": f"{fn_cost}:{fp_cost}",
        "t_final_cv": t_final,
        "test_cost": cost,
        "test_cost_per_obs": cost/len(y_te),
        "approval": approval,
        "bad_rate_approved": bad_rate_app,
        "bad_recall": bad_rec,
        "bad_precision": bad_prec,
        "FP": fp, "FN": fn
    }

cost_ratios = [1, 2, 3, 5, 10, 20]      # FN cost (FP=1)
approval_mins = [0.40, 0.50, 0.60]

rows = []
for a_min in approval_mins:
    for fn_cost in cost_ratios:
        rows.append(evaluate_setting(fp_cost=1, fn_cost=fn_cost, approval_min=a_min))

sens_full = pd.DataFrame(rows).sort_values(["approval_min", "test_cost"])
sens_full

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix

ts = np.linspace(0.01, 0.99, 99)

def cost_metrics(y_true, p, t, fp_cost, fn_cost):
    y_pred = (p >= t).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    cost = fp_cost*fp + fn_cost*fn
    approval = (y_pred == 0).mean()
    bad_rate_approved = fn / (tn + fn) if (tn + fn) else np.nan
    bad_recall = tp / (tp + fn) if (tp + fn) else np.nan
    bad_precision = tp / (tp + fp) if (tp + fp) else np.nan
    return cost, approval, bad_rate_approved, bad_recall, bad_precision

def select_t_constrained(y_true, p, fp_cost, fn_cost, approval_min):
    best = None
    for t in ts:
        cost, approval, *_ = cost_metrics(y_true, p, t, fp_cost, fn_cost)
        if approval >= approval_min:
            if best is None or cost < best["cost"]:
                best = {"t": t, "cost": cost, "approval": approval}
    if best is None:
        # fallback: massimizza approval, poi minimizza costo
        best = {"t": 0.99, "approval": -1, "cost": np.inf}
        for t in ts:
            cost, approval, *_ = cost_metrics(y_true, p, t, fp_cost, fn_cost)
            if (approval > best["approval"]) or (approval == best["approval"] and cost < best["cost"]):
                best = {"t": t, "cost": cost, "approval": approval}
    return float(best["t"])

def sensitivity_cv_row(model, X_tr, y_tr, X_te, y_te, fp_cost, fn_cost, approval_min):
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    fold_costs, fold_approvals, fold_bad_rates, fold_ts = [], [], [], []

    for idx_tr, idx_va in skf.split(X_tr, y_tr):
        X_train_fold, y_train_fold = X_tr.iloc[idx_tr], y_tr.iloc[idx_tr]
        X_valid_fold, y_valid_fold = X_tr.iloc[idx_va], y_tr.iloc[idx_va]

        model.fit(X_train_fold, y_train_fold)
        p_va = model.predict_proba(X_valid_fold)[:, 1]

        t_star = select_t_constrained(y_valid_fold, p_va, fp_cost, fn_cost, approval_min)
        fold_ts.append(t_star)

        cost, approval, bad_rate_app, *_ = cost_metrics(y_valid_fold, p_va, t_star, fp_cost, fn_cost)
        fold_costs.append(cost)
        fold_approvals.append(approval)
        fold_bad_rates.append(bad_rate_app)

    t_final = float(np.median(fold_ts))

    # Test (solo per reporting finale, NON per selezionare)
    model.fit(X_tr, y_tr)
    p_te = model.predict_proba(X_te)[:, 1]
    test_cost, test_approval, test_bad_rate_app, test_bad_recall, test_bad_precision = cost_metrics(
        y_te, p_te, t_final, fp_cost, fn_cost
    )

    return {
        "approval_min": approval_min,
        "FN:FP": f"{fn_cost}:{fp_cost}",
        "t_final_cv": t_final,
        "cv_cost_mean": float(np.mean(fold_costs)),
        "cv_cost_std": float(np.std(fold_costs, ddof=1)),
        "cv_approval_mean": float(np.mean(fold_approvals)),
        "cv_bad_rate_approved_mean": float(np.mean(fold_bad_rates)),
        "test_cost": float(test_cost),
        "test_approval": float(test_approval),
        "test_bad_rate_approved": float(test_bad_rate_app),
        "test_bad_recall": float(test_bad_recall),
        "test_bad_precision": float(test_bad_precision),
    }

cost_ratios = [1, 2, 3, 5, 10, 20]
approval_mins = [0.40, 0.50, 0.60]

rows = []
for a_min in approval_mins:
    for fn_cost in cost_ratios:
        rows.append(sensitivity_cv_row(logreg_nosens, X_tr, y_tr, X_te, y_te, fp_cost=1, fn_cost=fn_cost, approval_min=a_min))

sens_full_cv = pd.DataFrame(rows).sort_values(["approval_min", "cv_cost_mean"])
sens_full_cv

sens_full_cv.to_csv("/content/SENSITIVITY_COSTI_E_VINCOLI.csv", index=False)

import matplotlib.pyplot as plt

df = sens_full_cv.copy()
# estrai FN cost come numero
df["fn_cost"] = df["FN:FP"].str.split(":").str[0].astype(int)

plt.figure(figsize=(8,5))
for a in sorted(df["approval_min"].unique()):
    tmp = df[df["approval_min"]==a].sort_values("fn_cost")
    plt.plot(tmp["fn_cost"], tmp["t_final_cv"], marker="o", label=f"approval_min={a}")

plt.title("Sensitivity: soglia CV vs costo FN (FP=1)")
plt.xlabel("Costo FN (FP=1)")
plt.ylabel("Soglia selezionata (t_final_cv)")
plt.xticks(sorted(df["fn_cost"].unique()))
plt.legend()
plt.tight_layout()
plt.show()